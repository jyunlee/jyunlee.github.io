<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>Pop-Out Motion: 3D-Aware Image Deformation via Learning the Shape Laplacian"</title>
	<meta property="og:image" content="resources/teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Pop-Out Motion: 3D-Aware Image Deformation
	via Learning the Shape Laplacian"/>
	<meta property="og:description" content="We propose a framework that can deform an object in a 2D image as it exists in 3D space. Most existing methods for 3D-aware image manipulation are limited to (1) only changing the global scene information or depth, or (2) manipulating an object of specific categories. In this paper, we present a 3D-aware image deformation method with minimal restrictions on shape category and deformation type. While our framework leverages 2D-to-3D reconstruction, we argue that reconstruction is not sufficient for realistic deformations due to the vulnerability to topological errors. Thus, we propose to take a supervised learning-based approach to predict the shape Laplacian of the underlying volume of a 3D reconstruction represented as a point cloud. Given the deformation energy calculated using the predicted shape Laplacian and user-defined deformation handles (e.g., keypoints), we obtain bounded biharmonic weights to model plausible handle-based image deformation. In the experiments, we present our results of deforming 2D character and clothed human images. We also quantitatively show that our approach can produce more accurate deformation weights compared to alternative methods (i.e., mesh reconstruction and point cloud Laplacian methods)." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px"><p style="line-height:0">Pop-Out Motion:</p>
			<p style="line-height:0.6">3D-Aware Image Deformation via Learning the Shape Laplacian</p></span>

		<span style="font-size:24px"><p style="line-height:0.6">In CVPR 2022</p></span>
		&nbsp;
		<table align=center width=800px>
			<table align=center width=800px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:20px"><a href="https://scholar.google.com/citations?user=UaMiOq8AAAAJ&hl=en">Jihyun Lee</a>*<sup>1</sup></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:20px"><a href="https://mhsung.github.io">Minhyuk Sung</a>*<sup>1</sup></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:20px">Hyunjin Kim<sup>1</sup></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:20px"><a href="https://sites.google.com/view/tkkim/home">Tae-Kyun Kim</a><sup>1,2</sup></span>
						</center>
					</td>
	
				</tr>
			</table>

			<table align=center width=600px>
				<tr>
					<td align=center width=300px>
						<center>
							<span style="font-size:20px">      <sup>1</sup> KAIST</span>
						</center>
					</td>
					<td align=center width=300px>
						<center>
							<span style="font-size:20px"><sup>2</sup> Imperial College London</span>
						</center>
					</td>
				</tr>
			</table>

			<center>
				<span style="font-size:14px"><p style="line-height:0.2">(*: equal contributions)</p></span>
			</center>

			&nbsp;
			<table align=center width=400px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://arxiv.org/pdf/2203.15235.pdf'>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://youtu.be/gHxwHxIZiuM'>[Video]</a></span><br>
						</center>
					</td>

					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/jyunlee/Pop-Out-Motion'>[Code]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	&nbsp;
	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:860px" src="./resources/teaser.png"/>
					</center>
				</td>
			</tr>
		</table>
	</center>

	&nbsp;
	<hr>
	&nbsp;
	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				We propose a framework that can deform an object in a 2D image as it exists in 3D space. Most existing methods for 3D-aware image manipulation are limited to (1) only changing the global scene information or depth, or (2) manipulating an object of specific categories. In this paper, we present a 3D-aware image deformation method with minimal restrictions on shape category and deformation type. While our framework leverages 2D-to-3D reconstruction, we argue that reconstruction is not sufficient for realistic deformations due to the vulnerability to topological errors. Thus, we propose to take a supervised learning-based approach to predict the shape Laplacian of the underlying volume of a 3D reconstruction represented as a point cloud. Given the deformation energy calculated using the predicted shape Laplacian and user-defined deformation handles (e.g., keypoints), we obtain bounded biharmonic weights to model plausible handle-based image deformation. In the experiments, we present our results of deforming 2D character and clothed human images. We also quantitatively show that our approach can produce more accurate deformation weights compared to alternative methods (i.e., mesh reconstruction and point cloud Laplacian methods).
			</td>
		</tr>
	</table>
	<br>

	&nbsp;
	<hr>

	&nbsp;
	<center><h1>Video</h1></center>
	<p align="center">
		<iframe width="784" height="441" src="https://www.youtube.com/embed/gHxwHxIZiuM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
	</p>

	&nbsp;
	<hr>

	&nbsp;
	<center><h1>Approach</h1></center>

	<table align=center width=420px>
		<center>
			<tr>
				<td>
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=860px>
		<tr>
			<td align=center width=860px>
				<center>
					<td><img class="round" style="width:860px" src="./resources/method.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
				We introduce a neural network that can predict the shape Laplacian of the underlying volume of a 3D point cloud reconstructed from a 2D image — without directly converting the point cloud to a volume. Considering that the deformation energy can be discretized with the standard linear FEM Laplacian <i>LM<sup>−1</sup>L</i> (where <i>L</i> is a symmetric cotangent Laplacian matrix and <i>M</i> is a diagonal lumped mass matrix), we design our network to learn the matrices <i>L</i> and <i>M<sup>−1</sup></i> from the supervision obtained from a ground truth 3D mesh. The elements in the inverse mass matrix <i>M<sup>−1</sup></i>  are predicted for each individual point, while the elements of the cotangent Laplacian matrix <i>L</i> are predicted by taking pairs of the input points. We use a symmetric feature aggregation function for such pairs and also a weight module to enforce the output matrix <i>L</i> to be symmetric and sparse. In test time, we recover the deformation energy from the predicted <i>L</i> and <i>M<sup>−1</sup></i>  to compute bounded biharmonic weights with user-specified deformation handles. Since our method learns the shape Laplacian instead of the handle-dependent deformation weights, it can generalize well to arbitrary handle configurations.
				</td>
			</tr>
		</center>
	</table>
	<br>

	&nbsp;
	<hr>
	<table align=center width=600px>
		<center><h1>Paper and Supplementary Material</h1></center>
		<tr>
			<td><a href="https://arxiv.org/pdf/2203.15235.pdf"><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>
			<td><span style="font-size:12pt">Jihyun Lee*, Minhyuk Sung*, Hyunjin Kim, Tae-Kyun Kim.<br>
			(*: equal contributions)
			<td><b>Pop-Out Motion: 3D-Aware Image Deformation via Learning the Shape Laplacian.</b><br>
			In CVPR, 2022.<br>
			<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
			<span style="font-size:4pt"><a href=""><br></a>
			</span>

			<span style="font-size:14pt"><a href="./resources/bibtex.txt">[Bibtex]</a></span>
			</td>
		</tr>
	</table>
	<br>

	&nbsp;
	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					We would like to thank Duygu Ceylan for helpful discussions. This work is in part supported by KAIA grant (22CTAP-C163793-02) funded by the Korea government(MOLIT) and NST grant (CRC21011) funded by the Korea government(MSIT). M. Sung also acknowledges the support by NRF grant (2021R1F1A1045604) funded by the Korea government(MSIT), Technology Innovation Program (20016615) funded by the Korea government(MOTIE), and grants from the Adobe and KT corporations.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

