
<!DOCTYPE html>
<html lang="en">
  <head>
    <!-- Meta tags -->
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <!-- HTML Meta Tags -->
    <title>Im2Hands: Learning Attentive Implicit Representation of Interacting Two-Hand Shapes</title>
    <meta
      name="description"
      content="https://jyunlee.github.io/projects/im2hands/"
    />
    <meta
      property="og:image"
      content="data/fig_teaser.png"
    />
    <meta property="og:image:type" content="image/png" />
    <meta property="og:image:width" content="1704" />
    <meta property="og:image:height" content="636" />
    <meta property="og:type" content="website" />
    <meta
      property="og:title"
      content="Im2Hands: Learning Attentive Implicit Representation of Interacting Two-Hand Shapes"
    />
    <meta
      property="og:description"
      content="We present Implicit Two Hands (Im2Hands), the first neural implicit representation of two interacting hands. Unlike existing methods on two-hand reconstruction that rely on a parametric hand model and/or low-resolution meshes, Im2Hands can produce fine-grained geometry of two hands with high hand-to-hand and hand-to-image coherency. To handle the shape complexity and interaction context between two hands, Im2Hands models the occupancy volume of two hands - conditioned on an RGB image and coarse 3D keypoints - by two novel attention-based modules responsible for initial occupancy estimation and context-aware occupancy refinement, respectively. Im2Hands first learns per-hand neural articulated occupancy in the canonical space designed for each hand using query-image attention. It then refines the initial two-hand occupancy in the posed space to enhance the coherency between the two hand shapes using query-anchor attention. In addition, we introduce an optional keypoint refinement module to enable robust two-hand shape estimation from predicted hand keypoints in a single-image reconstruction scenario. We experimentally demonstrate the effectiveness of Im2Hands on two-hand reconstruction in comparison to related methods, where ours achieves state-of-the-art results."
    />
    <meta name="twitter:card" content="summary_large_image" />
    <meta
      name="twitter:title"
      content="Im2Hands: Learning Attentive Implicit Representation of Interacting Two-Hand Shapes"
    />
    <meta
      name="twitter:description"
      content="We present Implicit Two Hands (Im2Hands), the first neural implicit representation of two interacting hands. Unlike existing methods on two-hand reconstruction that rely on a parametric hand model and/or low-resolution meshes, Im2Hands can produce fine-grained geometry of two hands with high hand-to-hand and hand-to-image coherency. To handle the shape complexity and interaction context between two hands, Im2Hands models the occupancy volume of two hands - conditioned on an RGB image and coarse 3D keypoints - by two novel attention-based modules responsible for initial occupancy estimation and context-aware occupancy refinement, respectively. Im2Hands first learns per-hand neural articulated occupancy in the canonical space designed for each hand using query-image attention. It then refines the initial two-hand occupancy in the posed space to enhance the coherency between the two hand shapes using query-anchor attention. In addition, we introduce an optional keypoint refinement module to enable robust two-hand shape estimation from predicted hand keypoints in a single-image reconstruction scenario. We experimentally demonstrate the effectiveness of Im2Hands on two-hand reconstruction in comparison to related methods, where ours achieves state-of-the-art results."
    />
    <meta
      name="twitter:image"
      content="data/fig_teaser.png"
    />
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css"
      rel="stylesheet"
      integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3"
      crossorigin="anonymous"
    />
    <link
      href="https://fonts.googleapis.com/css?family=Lato|Varela+Round|Open+Sans"
      rel="stylesheet"
      type="text/css"
    />
    <link
      rel="icon"
      href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üñê</text></svg>"
    />
    <!-- Bootstrap JS -->
    <script
      src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"
      integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
      crossorigin="anonymous"
    ></script>
    <!-- Customized style -->
    <style>
      html * {
        color: #333;
        font-family: "Lato", sans-serif;
      }
      table.results td {
        color: #888;
        font-size: 90%;
      }
      .mtitle {
        margin-top: 0;
        margin-bottom: 0;
        font-family: "Varela Round", sans-serif;
        color: #4d4d4d;
        font-size: 50px;
        line-height: 80px;
        font-weight: 600;
        letter-spacing: 3px;
      }
      .msubtitle {
        margin-top: -30px;
        margin-bottom: -20px;
        font-size: 23px;
        line-height: 65px;
        letter-spacing: 2px;
      }
      .mneurips {
        color: #aaa;
        font-size: 20px;
      }
      .mauthors {
        font-size: 16px;
        font-weight: 400;
        line-height: 15px;
      }
      .mauthors_affiliation {
        margin-top: -12px;
        margin-bottom: 24px;
        font-family: "Open Sans", sans-serif;
        font-size: 12px;
        line-height: 15px;
      }
      .darker_bg {
        padding-top: 32px;
        padding-bottom: 32px;
        background-color: #f5f5f5;
      }
      .nav-link {
        color: #333;
      }
      .nav-link a:hover,
      a:hover,
      a:active {
        color: #888 !important;
      }
      .accordion-button:not(.collapsed) {
        color: inherit;
        background: #f5f5f5;
      }
      .accordion-button:not(.collapsed)::after {
        filter: brightness(0%) invert(70%);
      }
      .accordion-button:focus {
        box-shadow: inherit;
      }
    </style>
  </head>

  <body>
    <a
      href="https://github.com/jyunlee/Im2Hands"
      class="github-corner"
      aria-label="View source on GitHub"
      ><svg
        width="80"
        height="80"
        viewBox="0 0 250 250"
        style="
          fill: #151513;
          color: #fff;
          position: absolute;
          top: 0;
          border: 0;
          right: 0;
        "
        aria-hidden="true"
      >
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path
          d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
          fill="#fff"
          style="transform-origin: 130px 106px"
          class="octo-arm"
        ></path>
        <path
          d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
          fill="#fff"
          class="octo-body"
        ></path></svg></a
    ><style>
      .github-corner:hover .octo-arm {
        animation: octocat-wave 560ms ease-in-out;
      }
      @keyframes octocat-wave {
        0%,
        100% {
          transform: rotate(0);
        }
        20%,
        60% {
          transform: rotate(-25deg);
        }
        40%,
        80% {
          transform: rotate(10deg);
        }
      }
      @media (max-width: 500px) {
        .github-corner:hover .octo-arm {
          animation: none;
        }
        .github-corner .octo-arm {
          animation: octocat-wave 560ms ease-in-out;
        }
      }
    </style>
    <div class="container-fluid my-5 mx-auto" style="max-width: 1000px">
      <div class="row">

        <h1 class="display-8 text-center mtitle">&#129330; Im2Hands</h1>
        <h1 class="display-8 text-center msubtitle">
          Learning Attentive Implicit Representation of Interacting Two-Hand Shapes
        </h1>
      </div>

      <div class="row mt-2">
        <div class="display-8 text-center mneurips">CVPR 2023</div>
      </div>
      <br>

      <div
        class="row text-center mt-3 mx-auto mauthors"
        style="max-width: 800px"
      />
        <div class="col-md-3">
          <a href="https://jyunlee.github.io/"> Jihyun Lee </a>
        </div>
        <div class="col-md-3">
          <a href="https://mhsung.github.io/"> Minhyuk Sung </a>
        </div>
        <div class="col-md-3">
          <a href="https://honggyuchoi.github.io/ "> Honggyu Choi </a>
        </div>
        <div class="col-md-3">
          <a href="https://sites.google.com/view/tkkim/home"> Tae-Kyun (T-K) Kim </a>
        </div>
      </div>
      <div class="row text-center mt-3 mx-auto" style="max-width: 800px">
        <div class="col-md-3 d-none d-md-block mauthors_affiliation">
          KAIST
        </div>
        <div class="col-md-3 d-none d-md-block mauthors_affiliation">
          KAIST
        </div>
        <div class="col-md-3 d-none d-md-block mauthors_affiliation">KAIST</div>
        <div class="col-md-3 d-none d-md-block mauthors_affiliation">KAIST, Imperial College London</div>
      </div>
      <div class="row mt-2 text-center">
        <a class="nav-link col-3"></a>
        <a href="https://arxiv.org/abs/2302.14348" class="nav-link col-3">
          <svg style="width: 48px; height: 48px" viewBox="0 0 24 24">
            <path
              fill="currentColor"
              d="M16 0H8C6.9 0 6 .9 6 2V18C6 19.1 6.9 20 8 20H20C21.1 20 22 19.1 22 18V6L16 0M20 18H8V2H15V7H20V18M4 4V22H20V24H4C2.9 24 2 23.1 2 22V4H4M10 10V12H18V10H10M10 14V16H15V14H10Z"
            /></svg
          ><br />
          Paper
        </a>
        <a href="https://github.com/jyunlee/Im2Hands" class="nav-link col-3">
          <svg style="width: 48px; height: 48px" viewBox="0 0 24 24">
            <path
              fill="currentColor"
              d="M12,2A10,10 0 0,0 2,12C2,16.42 4.87,20.17 8.84,21.5C9.34,21.58 9.5,21.27 9.5,21C9.5,20.77 9.5,20.14 9.5,19.31C6.73,19.91 6.14,17.97 6.14,17.97C5.68,16.81 5.03,16.5 5.03,16.5C4.12,15.88 5.1,15.9 5.1,15.9C6.1,15.97 6.63,16.93 6.63,16.93C7.5,18.45 8.97,18 9.54,17.76C9.63,17.11 9.89,16.67 10.17,16.42C7.95,16.17 5.62,15.31 5.62,11.5C5.62,10.39 6,9.5 6.65,8.79C6.55,8.54 6.2,7.5 6.75,6.15C6.75,6.15 7.59,5.88 9.5,7.17C10.29,6.95 11.15,6.84 12,6.84C12.85,6.84 13.71,6.95 14.5,7.17C16.41,5.88 17.25,6.15 17.25,6.15C17.8,7.5 17.45,8.54 17.35,8.79C18,9.5 18.38,10.39 18.38,11.5C18.38,15.32 16.04,16.16 13.81,16.41C14.17,16.72 14.5,17.33 14.5,18.26C14.5,19.6 14.5,20.68 14.5,21C14.5,21.27 14.66,21.59 15.17,21.5C19.14,20.16 22,16.42 22,12A10,10 0 0,0 12,2Z"
            /></svg
          ><br />
          Code
        </a>
        <a class="nav-link col-3"></a>
      </div>


      <br>
      <div class="row">
        <div class="col-md-12 mt-2">
          <center>
            <embed src="data/fig_teaser.png" width=100% />
          </center>
        </div>
      </div>

      <div class="my-5">
        <h2>Presentation</h2>
        <div class="text-center mt-3">
          <div style="position: relative; padding-top: 56.25%">
            <iframe
              src="https://www.youtube.com/embed/hBSeN222Um4"
              allowfullscreen
              style="
                position: absolute;
                top: 0;
                left: 0;
                width: 100%;
                height: 100%;
              "
            ></iframe>
          </div>
        </div>
      </div>



      <div class="my-5">
        <h2>Abstract</h2>
        <p>
          We present Implicit Two Hands (Im2Hands), the first neural implicit representation of two interacting hands. Unlike existing methods on two-hand reconstruction that rely on a parametric hand model and/or low-resolution meshes, Im2Hands can produce fine-grained geometry of two hands with high hand-to-hand and hand-to-image coherency. To handle the shape complexity and interaction context between two hands, Im2Hands models the occupancy volume of two hands - conditioned on an RGB image and coarse 3D keypoints - by two novel attention-based modules responsible for initial occupancy estimation and context-aware occupancy refinement, respectively. Im2Hands first learns per-hand neural articulated occupancy in the canonical space designed for each hand using query-image attention. It then refines the initial two-hand occupancy in the posed space to enhance the coherency between the two hand shapes using query-anchor attention. In addition, we introduce an optional keypoint refinement module to enable robust two-hand shape estimation from predicted hand keypoints in a single-image reconstruction scenario. We experimentally demonstrate the effectiveness of Im2Hands on two-hand reconstruction in comparison to related methods, where ours achieves state-of-the-art results.
        </p>

        <center>
          <img src="data/fig_overall_architecture.png" width=95%>
        </center>
      </div>

      <div class="my-5">
        <h2>Video Results (InterHand2.6M)</h2>
        <div class="text-center mt-3">
          <div style="position: relative; padding-top: 56.25%">
            <iframe
              src="https://www.youtube.com/embed/3yNGSRz564A"
              allowfullscreen
              style="
                position: absolute;
                top: 0;
                left: 0;
                width: 100%;
                height: 100%;
              "
            ></iframe>
          </div>
        </div>
      </div>

      <div class="my-5">
        <h2>Image Results (InterHand2.6M)</h2>
        <p>
          Green boxes show penetrations, brown boxes show non-smooth shapes, and purple boxes show shapes with bad image alignment.
          Our method produces two-hand shapes with <b>better hand-to-image</b> and <b>hand-to-hand coherency</b>, <b>less penetrations</b>, and <b>a higher resolution</b>.        </p>

        <center>
          <img src="data/fig_supp_comparison_v1.png" width=100%>
        </center>
      </div>

      <div class="my-5">
        <h2>Generalizability Test (RGB2Hands and EgoHands)</h2>
        <div class="col-md-12 mt-2">
          <center>
            <img src="data/fig_rgb2hands.png" width=100%>
          </center>
        </div>
      </div>

      <div class="my-5">
        <h2>Citation</h2>
        <div class="col-md-12 mt-3">
          <pre>
@inproceedings{lee2023im2hands,
    title={Im2Hands: Learning Attentive Implicit Representation of Interacting Two-Hand Shapes},
    author={Lee, Jihyun and Sung, Minhyuk and Choi, Honggyu and Kim, Tae-Kyun},
    booktitle={CVPR},
    year={2023}
  }
      </pre>
        </div>
      </div>

      <div class="my-5">
        <h2>Acknowledgement</h2>
        <p>
This work is in part sponsored by NST grant (CRC 21011, MSIT) and KOCCA grant (R2022020028, MCST). Minhyuk Sung acknowledges the support of the NRF grant (RS-2023-00209723) and IITP grant (2022-0-00594) funded by the Korean government (MSIT), and grants from Adobe, ETRI, KT, and Samsung Electronics.
      </p>
      </div>

<!--
      <div class="my-5">
        <h2>Acknowledgement</h2>
        <p>
          We would like to thank Zhengqi Li and Keunhong Park for valuable
          feedback and discussions; Matthew Tancik and Ethan Weber for
          proofreading. We are also grateful to our pets: Sriracha, Haru, and
          Mochi, for being good during capture. This project is generously
          supported in part by the CONIX Research Center, sponsored by DARPA, as
          well as the BDD and BAIR sponsors.
        </p>
      </div>
-->
      <div class="text-center">
        <hr />
        <small
          >Inspired by
          <a href="https://grail.cs.washington.edu/projects/humannerf/"
            >HumanNeRF</a
          >
        </small>
      </div>
    </div>
  </body>
</html>
